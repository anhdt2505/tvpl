{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'pyarrow'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpq\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyarrow'"]}],"source":["import json\n","import os\n","import pyarrow.parquet as pq\n","import requests\n","import bs4\n","import urllib.parse\n","import hashlib\n","import pandas as pd\n","import regex\n","from unidecode import unidecode\n","from random import choice\n","import logging"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def check_dir_exist():\n","    # Check if directory data exists\n","    if not os.path.isdir('data'):\n","        os.mkdir('data')\n","    \n","    print('Folder data created')\n","\n","def append_to_parquet_file(df: pd.DataFrame, file_path: str):\n","    # Append dataframe into parquet file if file exists\n","    if os.path.exists(file_path):\n","        df.to_parquet(file_path, engine='fastparquet', append=True)\n","        \n","        print(f\"Data appended to {file_path}\")\n","\n","    else:\n","        df.to_parquet(file_path, engine='fastparquet')\n","        \n","        print(f\"New parquet file created at {file_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["def get_md5(s):\n","    m = hashlib.md5()\n","    m.update(s.encode('utf-8'))\n","    return m.hexdigest()\n","\n","\n","class TVPLCrawler():\n","    BATCH_SIZE = 50\n","    CRAWLER = 'tungks1'\n","    OUTPUT_PATH = 'output.json'\n","    PROXY_URL = 'https://api.proxyscrape.com/v2/?request=displayproxies&protocol=http&timeout=10000&country=all&ssl=all&anonymity=all'\n","\n","    def __init__(self):\n","        self.name = self.__class__.__name__\n","        self.log = logging.getLogger(self.name)\n","        self.log.setLevel(logging.INFO)\n","\n","    def crawl_link(self, file_path):\n","        # Lay link, tieu de, va ngay ban hanh cua tat ca van ban\n","        url = 'https://thuvienphapluat.vn/page/tim-van-ban.aspx?keyword=&area=0&match=True&type=0&status=0&signer=0&sort=1&lan=1&scan=0&org=0&fields=&page='\n","        page = 3000\n","        limit = page + 3000\n","\n","        while True:\n","            page_url = url + str(page)\n","            content = requests.get(page_url)\n","            soup = bs4.BeautifulSoup(content.text, \"html.parser\")\n","            links = soup.select(\".content-0\") + soup.select(\".content-1\")\n","\n","            if len(links) == 0:\n","                break\n","\n","            for link in links:\n","                title = link.select_one('.nqTitle')\n","                if title:\n","                    link_url = title.select_one('a')\n","                    if link_url:\n","                        link_url = link_url.get('href')\n","                        title = title.text.strip()\n","                        date = link.select_one('.right-col > p')\n","                        if date:\n","                            date = date.text\n","                            if 'Ban hành' in date:\n","                                date = date.split(':')[-1].strip()\n","                        else:\n","                            date = ''\n","\n","                        item = {'_id': get_md5(link_url),\n","                                'title': title,\n","                                'link': link_url,\n","                                'date': date,\n","                                'crawler': None,\n","                                'status': 'not_crawled'}\n","\n","                        # append dictionary into json file\n","                        item_df = pd.DataFrame([item])\n","                        append_to_parquet_file(item_df, file_path)\n","\n","            self.log.info(f'Finish page {page}')\n","\n","            if page > limit:\n","                break\n","\n","            page += 1\n","    \n","    # Lay noi dung chi tiet cua cac van ban\n","    def crawl_html(self, link, proxy=None):\n","        item = {'link': link}\n","\n","        if proxy:\n","            req = requests.get(link, proxies={'http': proxy})\n","        else:\n","            req = requests.get(link)\n","        print('Done')\n","\n","        soup = bs4.BeautifulSoup(req.text, 'html.parser')\n","\n","        item['title'] = soup.select_one('#divThuocTinh > h1')\n","        if item['title']:\n","            item['title'] = item['title'].text.strip()\n","\n","        thuoc_tinh_html = soup.select_one('#divThuocTinh > table')\n","        if thuoc_tinh_html:\n","            item['thuoc_tinh_html'] = str(thuoc_tinh_html)\n","        else:\n","            item['thuoc_tinh_html'] = ''\n","\n","        tom_tat_html = soup.select_one('.Tomtatvanban')\n","        if tom_tat_html:\n","            item['tom_tat_html'] = str(tom_tat_html)\n","        else:\n","            item['tom_tat_html'] = None\n","\n","        noi_dung_html = soup.select_one('.content1')\n","        if noi_dung_html:\n","            item['noi_dung_html'] = str(noi_dung_html)\n","        else:\n","            noi_dung_html = ''\n","\n","        # Lay phan thuoc tinh từ html và tach thanh cac gia tri\n","        if thuoc_tinh_html:\n","            thuoc_tinh_text = thuoc_tinh_html.text.replace('\\r', '').replace(':', '').split('\\n')\n","            thuoc_tinh_text = [t.strip() for t in thuoc_tinh_text if t.strip()]\n","            thuoc_tinh = {unidecode(a).lower().replace(' ', '_'): b for a, b in\n","                          zip(thuoc_tinh_text[::2], thuoc_tinh_text[1::2])}\n","            item.update(thuoc_tinh)\n","\n","        # Lay text phan tom tat\n","        if tom_tat_html:\n","            tom_tat_text = tom_tat_html.text\n","            item['tom_tat'] = tom_tat_text.strip()\n","\n","        # Lay bang, chi lay nhung bang co trong noi_dung_html, bo 2 bang o dau\n","        df_list = pd.read_html(item['noi_dung_html'], header=0)\n","        dict_result = {}\n","        for i in range(len(df_list)):\n","            dict_result[f'bang_{i}'] = df_list[i].to_dict(orient=\"records\")\n","        item['danh_sach_bang'] = dict_result\n","\n","        # Lay text phan noi dung\n","        if noi_dung_html:\n","            # Thay tag <table>...</table> thanh <table>bang_1</table>\n","            noi_dung_html = item['noi_dung_html']\n","            for name, df in dict_result.items():\n","                #         df_string = json.dumps(df)\n","                noi_dung_html = regex.sub(r\"<table[^>]*>(.|\\n)*?</table>\",\n","                                          f'&lt;jsontable name=\"{name}\"&gt; &lt;/jsontable&gt;', noi_dung_html, count=1)\n","\n","            # Lay text phan noi dung\n","            noi_dung_html = bs4.BeautifulSoup(noi_dung_html, 'html.parser')\n","            noi_dung_text = noi_dung_html.text\n","            item['noi_dung'] = noi_dung_text.strip()\n","\n","            # Lay cac link trich dan trong noi dung\n","            links_tag = noi_dung_html.select('a[href]')\n","            links = [l.get('href') for l in links_tag]\n","            names = [l.get('title') for l in links_tag]\n","            links = {name: urllib.parse.urljoin(item['link'], link) for name, link in zip(names, links) if\n","                     (name is not None) and (link is not None)}\n","            item['van_ban_duoc_dan'] = links\n","\n","        return item\n","\n","    def crawl(self):\n","        count = 0\n","        success = 0\n","        fail = 0\n","\n","        parquet_file = pq.ParquetFile('data/url1.parquet')\n","\n","        for batch in parquet_file.iter_batches(batch_size=5):\n","            print(\"RecordBatch\")\n","            batch_df = batch.to_pandas()\n","            \n","            proxy_list = requests.get(self.PROXY_URL)\n","            if proxy_list.status_code == 200:\n","                proxy_list = proxy_list.text.split('\\r\\n')\n","                proxy = choice(proxy_list)\n","            else:\n","                proxy = None\n","\n","            items = []\n","            \n","            for _, link in batch_df.iterrows():\n","                try:\n","                    item = self.crawl_html(link['link'], proxy)\n","                    item['_id'] = link['_id']\n","                    item['title'] = link['title']\n","                    items.append(item)\n","\n","                    # update status, crawler in url.parquet\n","                    link['status'] = 'crawled'\n","                    link['crawler'] = self.CRAWLER\n","                    success += 1\n","                \n","                except Exception as e:\n","                    link['status'] = 'failed'\n","                    link['crawler'] = self.CRAWLER\n","                    fail += 1\n","                \n","                count += 1\n","            \n","            # insert crawled items into crawled_items.parquet\n","            items_df = pd.DataFrame(items)\n","            append_to_parquet_file(items_df, 'data/crawled_items.parquet')\n","\n","            break    \n","        # self.log.info(f'Done {count} items, {success} success, {fail} fail')\n","\n","if __name__ == '__main__':\n","    url_file_path = 'data/url1.parquet'\n","\n","    crawler = TVPLCrawler()\n","    crawler.crawl_link(url_file_path)\n","#     crawler.crawl()\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3.9.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":4}
